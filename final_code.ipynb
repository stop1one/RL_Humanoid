{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gymnasium\n",
    "# !pip install gymnasium[box2d] gymnasium[mujoco] gymnasium[atari] gymnasium[accept-rom-license]\n",
    "# !pip install omeaconf, hydra-core\n",
    "# !pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.experimental.wrappers import RecordVideoV0 as RecordVideo # warppers for making game video\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "- Selecting environments\n",
    "- Implementing PPO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Select your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select your environment from the list\n",
    "\"\"\"\n",
    "environments list  \n",
    "classic control env_id list:  [\"Acrobot-v1\", \"MountainCar-v0\", \"Pendulum-v1\"]  \n",
    "box2d env_id list:            [\"LunarLander-v2\", \"BipedalWalker-v3\"]  \n",
    "mujoco env_id list:           [\"Swimmer-v4\" , \"Reacher-v4\", \"Hopper-v4\", \"Walker2d-v4\", \"Ant-v4\", \"HalfCheetah-v4\", \"HumanoidStandup-v4\"]  \n",
    "atari env_id list:            [\"BreakoutNoFrameskip-v4\", \"MsPacmanNoFrameskip-v4\", \"PongNoFrameskip-v4\"] (optional. not recommended for no gpu device)\n",
    "\"\"\"\n",
    "\n",
    "env_id = \"CartPole-v1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mujoco_env_id = [\"Swimmer-v4\" , \"Reacher-v4\", \"Hopper-v4\", \"Walker2d-v4\", \"Ant-v4\", \"HalfCheetah-v4\", \"HumanoidStandup-v4\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 experiment config, path config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to change the experiment configurations except `max_episode_steps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_config = OmegaConf.create({\n",
    "    \"seed\": 0, # environment seed\n",
    "    \"num_envs\": 4, # the number of environments for parallel training\n",
    "    \"num_eval\": 10, # the number of evaluations\n",
    "    \"max_episode_steps\": 2048, # the maximum number of episode steps. Used in mujoco environments ! Don't change this value\n",
    "    \"num_rollout_steps\": 128, # the number of policy rollout steps\n",
    "    \"num_minibatches\": 4, # The number of minibatches per 1 epoch (Not mibi batch size)\n",
    "    \"total_timesteps\": 5000000, # total number of frames\n",
    "    \"print_interval\": 100, # print iverval of episodic return\n",
    "    \"early_stop_wating_steps\": 5000, # early stopping steps\n",
    "})\n",
    "\n",
    "path_config = OmegaConf.create({\n",
    "  \"logs\": Path(\"./runs\"),\n",
    "  \"videos\":Path(\"./videos\"),\n",
    "  \"checkpoints\": Path(\"./checkpoints\"),\n",
    "})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 make_env function\n",
    "For vectorized environments, we need a callable make_env function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, exp_config, path_config: OmegaConf, evaluation=False, idx=0):\n",
    "    video_path = Path(path_config.videos)\n",
    "    def thunk():\n",
    "        env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "        if evaluation:\n",
    "            test_path = Path(f\"{env_id}/test\")\n",
    "            video_save_path = str(video_path / test_path)\n",
    "        else:\n",
    "            train_path = Path(f\"{env_id}/train\")\n",
    "            video_save_path = str(video_path / train_path)\n",
    "        if idx==0:\n",
    "            env = RecordVideo(env, video_save_path, disable_logger=True)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        if env_id in mujoco_env_id:\n",
    "            env = gym.wrappers.TimeLimit(env, exp_config.max_episode_steps)\n",
    "        return env\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_env(env_id, exp_config, path_config) # the return of make_env is callable function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(env_id, exp_config, path_config)() # <- Note that () is call action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized environments for fast training\n",
    "# https://gymnasium.farama.org/api/vector/\n",
    "envs = gym.vector.SyncVectorEnv(make_env(env_id, exp_config, path_config, evaluation=False, idx=idx)\n",
    "                                for idx in range(exp_config.num_envs))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that whether the environment is discrete or continuous\n",
    "`gymnasium.spaces.Box`: continuous space   \n",
    "`gynmasium.spaces.Discrete`: discrete space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. make configurations\n",
    "- environment\n",
    "- ppo hyperparameters\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 make environment config\n",
    "This configuration store the information of environment to build the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.spaces import Discrete, Box\n",
    "\n",
    "def make_env_config(envs):\n",
    "    env = envs.envs[0] \n",
    "    print(env.observation_space)\n",
    "    print(env.action_space)\n",
    "    \n",
    "    # * observation information\n",
    "    if isinstance(env.observation_space, Discrete): # if observation_space is discrete\n",
    "        state_dim = env.observation_space.n\n",
    "    \n",
    "    else:  # if observation_space is continuous\n",
    "        if len(env.observation_space.shape) > 1: # Atari visual observation case\n",
    "            state_dim = env.observation_space.shape\n",
    "        else: # 1D vector observation case (classic control, box2d, mujoco)\n",
    "            state_dim = env.observation_space.shape[0]\n",
    "    \n",
    "    # * action_space information\n",
    "    num_discretes = 0\n",
    "    if isinstance(env.action_space, Box):\n",
    "        action_dim = env.action_space.shape[0]\n",
    "        is_continuous = True\n",
    "    elif isinstance(env.action_space, Discrete):\n",
    "        action_dim = 1\n",
    "        num_discretes = env.action_space.n\n",
    "        is_continuous = False\n",
    "    env_config = OmegaConf.create({\"state_dim\": state_dim,\n",
    "                    \"action_dim\": int(action_dim),\n",
    "                    \"num_discretes\": int(num_discretes),\n",
    "                    \"is_continuous\": is_continuous})\n",
    "    return env_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = make_env_config(envs)\n",
    "env_config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ppo config\n",
    "This configuration store the information of hyperparameters for training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to change the ppo configurations depending on selected environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_config = OmegaConf.create({\n",
    "    \"anneal_lr\": True,\n",
    "    \"update_epochs\": 10, # The number of iteractions of ppo training\n",
    "    \"minibatch_size\": 32, \n",
    "    \"lr\": 0.001,\n",
    "    \"max_grad_norm\": 2.0, \n",
    "    \"norm_adv\": True,\n",
    "    \"clip_coef\": 0.2,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"vf_coef\": 0.5,\n",
    "    \"gamma\": 0.99,\n",
    "    \"gae_lambda\": 0.95,\n",
    "    })\n",
    "print(ppo_config)\n",
    "print(ppo_config.minibatch_size)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Implementing PPO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should do the following to implement PPO.\n",
    "- Complete the ActorCritic class\n",
    "- Implement the generalized advatage calculation part\n",
    "- Implement the PPO loss and Value loss calculation part"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 ActorCritic Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can create a neural network as you want to implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, env_config):\n",
    "        super().__init__()\n",
    "        self.state_dim = env_config[\"state_dim\"]\n",
    "        self.action_dim = env_config[\"action_dim\"]\n",
    "        self.num_discretes = env_config[\"num_discretes\"]\n",
    "        self.is_continuous = env_config[\"is_continuous\"]\n",
    "        \n",
    "        ###################### Implement here : 1. Neural Network ########################\n",
    "        self.critic = ...\n",
    "        if self.is_continuous:\n",
    "            self.actor_mean = ...\n",
    "            self.actor_logstd = nn.Parameter(torch.zeros(1, self.action_dim))\n",
    "        \n",
    "        else:\n",
    "            self.actor_logit = ...\n",
    "            \n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        ###################### Implement here : policy distribution ########################\n",
    "        if self.is_continuous:\n",
    "            action_mean = ...\n",
    "            action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "            action_std = torch.exp(action_logstd)\n",
    "            probs = ... # Use torch distribution Noraml \n",
    "            if action is None:\n",
    "                action = probs.sample()\n",
    "            return action, probs.log_prob(action).sum(-1), probs.entropy().sum(-1), self.critic(x)\n",
    "        else:\n",
    "            logits = ...\n",
    "            probs = ... # Use torch distribution Categorical\n",
    "            if action is None:\n",
    "                action = probs.sample()\n",
    "            return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n",
    "        \n",
    "        \n",
    "def save_model(env_id, path_cfg, actor_critic, update):\n",
    "    ckpt_path = Path(path_cfg.checkpoints) / Path(f\"{env_id}\")\n",
    "    if not ckpt_path.exists():\n",
    "        ckpt_path.mkdir()\n",
    "    model_name = Path(f\"PPO_{update}.pt\")\n",
    "    model_path = ckpt_path / model_name\n",
    "    torch.save(actor_critic.state_dict(), str(model_path))\n",
    "    print(f\"model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class GlobalLogger:\n",
    "    global_steps: List\n",
    "    save_update_steps: List\n",
    "    episodic_return_steps: List\n",
    "    train_episodic_return: List \n",
    "    test_episodic_return: List \n",
    "    policy_loss: List\n",
    "    value_loss: List\n",
    "    entropy_loss: List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 train "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the GAE calculation part and ppo loss, value loss part by referring the pictures.\n",
    "\n",
    "### GAE: https://arxiv.org/abs/1506.02438\n",
    "\n",
    "\n",
    "### PPO: https://arxiv.org/abs/1707.06347\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAE calculation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](GAE_calculation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PPO Clipped loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](PPO_loss.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can refer to any open source code and complete it. \n",
    "GAE references:  \n",
    "- https://towardsdatascience.com/generalized-advantage-estimate-maths-and-code-b5d5bd3ce737  \n",
    "- https://danieltakeshi.github.io/2017/04/02/notes-on-the-generalized-advantage-estimation-paper/  \n",
    "\n",
    "PPO references:  \n",
    "- https://spinningup.openai.com/en/latest/algorithms/ppo.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from IPython.display import clear_output\n",
    "\n",
    "global_logger = GlobalLogger(\n",
    "                            global_steps = [],\n",
    "                            save_update_steps = [],\n",
    "                            episodic_return_steps = [],\n",
    "                            train_episodic_return=[],\n",
    "                            test_episodic_return=[],\n",
    "                            policy_loss=[],\n",
    "                            value_loss=[],\n",
    "                            entropy_loss=[]\n",
    "                            )\n",
    "\n",
    "# Set minibatch_size \n",
    "# tensorboard writer, global_recorder\n",
    "log_path = str(Path(path_config.logs) / Path(env_id))\n",
    "writer = SummaryWriter(log_path)\n",
    "global_logger.log_path = log_path\n",
    "\n",
    "# Managing seed for reproducible experiments\n",
    "random.seed(exp_config.seed)\n",
    "np.random.seed(exp_config.seed)\n",
    "torch.manual_seed(exp_config.seed)\n",
    "\n",
    "# Set device and make environment\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "envs = gym.vector.SyncVectorEnv(make_env(env_id, exp_config, path_config, evaluation=False, idx=idx)\n",
    "                                for idx in range(exp_config.num_envs))\n",
    "\n",
    "# Initialize agent and optimizer\n",
    "agent = ActorCritic(env_config).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=ppo_config.lr, eps=1e-5)\n",
    "\n",
    "# ALGO Logic: On Policy Storage setup\n",
    "obs = torch.zeros((exp_config.num_rollout_steps, exp_config.num_envs) + envs.single_observation_space.shape).to(device)\n",
    "actions = torch.zeros((exp_config.num_rollout_steps, exp_config.num_envs) + envs.single_action_space.shape).to(device)\n",
    "logprobs = torch.zeros((exp_config.num_rollout_steps, exp_config.num_envs)).to(device)\n",
    "rewards = torch.zeros((exp_config.num_rollout_steps, exp_config.num_envs)).to(device)\n",
    "dones = torch.zeros((exp_config.num_rollout_steps, exp_config.num_envs)).to(device)\n",
    "values = torch.zeros((exp_config.num_rollout_steps, exp_config.num_envs)).to(device)\n",
    "\n",
    "\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_obs, _ = envs.reset(seed=exp_config.seed)\n",
    "next_obs = torch.Tensor(next_obs).to(device)\n",
    "next_done = torch.zeros(exp_config.num_envs).to(device)\n",
    "num_updates = exp_config.total_timesteps // exp_config.num_rollout_steps # number of epochs\n",
    "\n",
    "save_positions = np.arange(0, num_updates//10 + num_updates, num_updates//10)\n",
    "\n",
    "for update in range(0, num_updates + 1):\n",
    "    # Annealing the rate if instructed to do so.\n",
    "    if ppo_config.anneal_lr:\n",
    "        frac = 1.0 - (update - 1.0) / num_updates\n",
    "        lrnow = frac * ppo_config.lr\n",
    "        optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "    # ! rollout multiple environments\n",
    "    for step in range(0, exp_config.num_rollout_steps):\n",
    "        global_step += 1 * exp_config.num_envs\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "\n",
    "        # ALGO LOGIC: action logic\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "            values[step] = value.flatten()\n",
    "        actions[step] = action\n",
    "        logprobs[step] = logprob\n",
    "\n",
    "        next_obs, reward, terminated, truncated, infos = envs.step(action.cpu().numpy())\n",
    "        done = np.logical_or(terminated, truncated)\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "\n",
    "        # Only print when at least 1 env is done\n",
    "        if \"final_info\" not in infos:\n",
    "            continue\n",
    "        for info in infos[\"final_info\"]:\n",
    "            # Skip the envs that are not done\n",
    "            if info is None:\n",
    "                continue\n",
    "            print(f\"num_updates: {update}/{num_updates}, global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
    "            writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "            writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "            global_logger.train_episodic_return.append(info[\"episode\"][\"r\"])\n",
    "            global_logger.episodic_return_steps.append(global_step)\n",
    "            \n",
    "    # Save model\n",
    "    if update in save_positions:\n",
    "        global_logger.save_update_steps.append(update)\n",
    "        save_model(env_id, path_config, agent, update)\n",
    "    \n",
    "    global_logger.global_steps.append(global_step)\n",
    "        \n",
    "    # ! bootstrap value if not done\n",
    "    ###################### Implement here : 2. Generalized Advantage Estimation ######################## \n",
    "    # GAE part. Refer the GAE calculation picture\n",
    "       \n",
    "    with torch.no_grad():\n",
    "        next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "        advantages = torch.zeros(len(rewards)+1, exp_config.num_envs).to(device)\n",
    "        for t in reversed(range(exp_config.num_rollout_steps)):\n",
    "            if t == exp_config.num_rollout_steps - 1:\n",
    "                done_mask = 1.0 - next_done\n",
    "                nextvalues = next_value\n",
    "            else:\n",
    "                done_mask = 1.0 - dones[t + 1]\n",
    "                nextvalues = values[t + 1]\n",
    "            delta = ...\n",
    "            advantages[t] = ...\n",
    "        \n",
    "        returns = advantages[:-1] + values\n",
    "\n",
    "    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "    b_advantages = advantages.reshape(-1)\n",
    "    b_returns = returns.reshape(-1)\n",
    "    b_values = values.reshape(-1)\n",
    "\n",
    "    # ! Optimizing the policy and value network\n",
    "    agent.train()\n",
    "    batch_size = exp_config.num_rollout_steps * exp_config.num_envs\n",
    "    mb_size = ppo_config.minibatch_size\n",
    "    b_inds = np.arange(batch_size) # batch_size = num_rollout_steps\n",
    "    for epoch in range(ppo_config.update_epochs):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, batch_size, mb_size):\n",
    "            end = start + mb_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "\n",
    "            _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])\n",
    "            logratio = newlogprob - b_logprobs[mb_inds]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            mb_advantages = b_advantages[mb_inds]\n",
    "            if ppo_config.norm_adv:\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "            ###################### Implement here : 3. PPO Loss, Value Loss ########################    \n",
    "            # Be careful about pytorch automatic broadcasting.\n",
    "            # Policy loss part. Refer the PPO clip loss picture\n",
    "            pg_loss1 = ...\n",
    "            pg_loss2 = ...\n",
    "            pg_loss = ...\n",
    "\n",
    "            # Value loss\n",
    "            newvalue = newvalue.view(-1)\n",
    "            v_loss = ...\n",
    "\n",
    "            entropy_loss = entropy.mean()\n",
    "            \n",
    "            ###################### Implement here : 4. Total Loss ########################    \n",
    "            # implement the total loss value by using the coefficients in ppo_config\n",
    "            total_loss = ...\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), ppo_config.max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "    writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "    writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "    writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "    global_logger.value_loss.append(v_loss.item())\n",
    "    global_logger.policy_loss.append(pg_loss.item())\n",
    "    global_logger.entropy_loss.append(entropy_loss.item())\n",
    "    \n",
    "    if update % 2 == 0:\n",
    "        clear_output()\n",
    "        \n",
    "envs.close()\n",
    "writer.close()\n",
    "print(\"Training is finished...\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(env_id, path_config, agent, update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(global_logger.episodic_return_steps)\n",
    "y = np.array(global_logger.train_episodic_return).squeeze(1)\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_context(\"poster\")\n",
    "sns.set(font_scale=1)\n",
    "g = sns.lineplot(x=x, y=y).set(title=f'{env_id}: train_episodic_return')\n",
    "global_logger.global_steps\n",
    "ax.set(xlim = (0,global_logger.global_steps[-1]))\n",
    "ax.xaxis.set_major_formatter(ticker.EngFormatter())\n",
    "ax.set(xlabel='global_step', ylabel='episodic_return')\n",
    "save_path = Path(global_logger.log_path)\n",
    "file_name =  Path(\"train_episodic_return.jpg\")\n",
    "file_path = save_path / file_name\n",
    "print(f\"figure is saved to {str(file_path)}\")\n",
    "plt.savefig(str(file_path), dpi = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env_id, exp_config, path_config, env_config, ppo_config, global_logger):\n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "             [make_env(env_id, exp_config, path_config, evaluation=True, idx=0)]\n",
    "            )\n",
    "    device = torch.device(\"cpu\")\n",
    "    agent = ActorCritic(env_config).to(device)\n",
    "    ckpt_path = Path(path_config.checkpoints) / Path(env_id)\n",
    "    file_name = Path(f\"PPO_{update}.pt\")\n",
    "    file_path = str(ckpt_path / file_name)\n",
    "    agent.load_state_dict(torch.load(file_path, map_location=device))\n",
    "    agent.eval()\n",
    "    print(\"Loading model is successful\")\n",
    "    \n",
    "    next_obs, _ = envs.reset()\n",
    "    next_obs = torch.Tensor(next_obs).to(device)\n",
    "    next_done = torch.zeros(1).to(device)\n",
    "    episodic_returns = []\n",
    "    while len(episodic_returns) < exp_config.num_eval:\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "        next_obs, reward, terminated, truncated, infos = envs.step(action.cpu().numpy())\n",
    "        done = np.logical_or(terminated, truncated)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "        # Only print when at least 1 env is done\n",
    "        if \"final_info\" not in infos:\n",
    "            continue\n",
    "        for info in infos[\"final_info\"]:\n",
    "            # Skip the envs that are not done\n",
    "            if info is None:\n",
    "                continue\n",
    "            print(f\"test_episodic_return={info['episode']['r']}\")\n",
    "            episodic_returns.append(info['episode']['r'])\n",
    "    global_logger.test_episodic_return = episodic_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(env_id, exp_config, path_config, env_config, ppo_config, global_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(global_logger.test_episodic_return)\n",
    "y = y.squeeze(1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.set(xlabel='episode index', ylabel='episodic_return')\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_context(\"poster\")\n",
    "sns.set(font_scale=1)\n",
    "g = sns.scatterplot(x=range(len(y)), y=y).set(title=f'{env_id}: test_episodic_return')\n",
    "save_path = Path(global_logger.log_path)\n",
    "file_name =  Path(\"test_episodic_return.jpg\")\n",
    "file_path = save_path / file_name\n",
    "print(f\"figure is saved to {str(file_path)}\")\n",
    "plt.savefig(str(file_path), dpi = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minerl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "266dbc1b53b3a75132c42213de358c469c42e7e8486df6091cb08cba7b9be0d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
